{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "Author: Jacob McCabe\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will take a look at K-Nearest Neighbors algorithm. The main ideas covered will include:\n",
    "\n",
    "1. Explanation the algorithm\n",
    "2. Usage for real problems\n",
    "3. Example using `KNeighborsClassifier` from `scikit-learn`\n",
    "\n",
    "## What is K-Nearest Neighbors?\n",
    "\n",
    "The K-Nearest Neighbors algorithm is a non-parametric, supervised learning classifier. It assumes there is proximity of points to make predictions about the grouping of individual data points, i.e. similar points will be located close to each other. In the algorithm, $k > 0$ represents the number of neighbors to use for a point. \n",
    "\n",
    "To visualize the process, consider a sphere centered on a point $x$ that we want to label/classify. That sphere contains $k$ points closest to $x$ (typically Euclidean distance) that have a defined label. To label our point $x$, it will use the most often represented class in the $k$ points around it. For regression problems, the mean of the $k$ nearest neighbors is used.\n",
    "\n",
    "## Usage for real problems\n",
    "\n",
    "KNN is useful in a variety of situations, typically for classification problems.\n",
    "- Preprocessing data to fill missing values\n",
    "- Automatic recommendations\n",
    "- Pattern recognition\n",
    "\n",
    "### Pros\n",
    "\n",
    "While there are not a ton of advantages to this algorithm, it is easy to implement. This also makes it very common for teaching to beginners. Another advantage is that unlike many other models, there is only two hyperparameters: $k$ and the distance metric. This means that fine-tuning can be much easier since the search grid is typically more narrow than other ML algorithms.  \n",
    "\n",
    "### Cons\n",
    "\n",
    "KNN is a 'lazy algorithm'. This means that it has to hold all data points in memory and is inefficient for large datasets. As many datasets continue to grow larger and larger, it becomes increasingly more difficult to run KNN efficiently. There are ways to help combat this issue, but they won't be discussed here. It also often is a victim of dimensionality, and so with more features it often is more likely to misclassify points.\n",
    "\n",
    "## Example using `KNeighborsClassifier` from `scikit-learn` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
