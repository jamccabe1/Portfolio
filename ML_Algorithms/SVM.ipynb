{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs)\n",
    "\n",
    "Author: Jacob McCabe\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook will take a look at Support Vector Machines for the purpose of classification. The main ideas covered will include:\n",
    "\n",
    "1. The main idea behind SVMs\n",
    "2. Primal Problem\n",
    "3. Dual Problem\n",
    "4. Kernel Functions\n",
    "5. Multiclass SVMs\n",
    "6. Example using `SVC` from `skikit-learn`\n",
    "\n",
    "## What is a SVM?\n",
    "\n",
    "Support Vector Machines are a category of non-probabilistic, supervised learning algorithms for binary classification. They work by fitting a hyperplane between classes, and are trained by finding the hyperplane that maximizes the margin. Margin is the distance between the decision boundary and closest data point. Those points that are closest to the decision boundary and thus define the margin are referred to as support vectors.\n",
    "\n",
    "<img src=\"images\\svm_visualization.png\" height='50%' width='50%'>\n",
    "\n",
    "\n",
    "One of the important properties of this class of algorithms is that they are equivalent to solving a convex optimization problem, so any local solution is also a global optimum. When it comes to solving for the optimum, there are a few different flavors of SVMs.\n",
    "\n",
    "If the classes are linearly seperable, we call this a **hard margin**. This is the case where all of class 0 falls on one side of the hyperplane and all of class 1 is on the other side. If the classes are not linearly seperable, it is a **soft margin**. In this case there are points in a class that fall on the wrong side of the hyperplane and a hyperparameter $C$ is needed to penalize the model based on how far over the boundary a point is. The soft margin problem also requires slack variables, $\\xi$. These are scalar quantities representing how much a specific point has intruded into the margin. \n",
    "\n",
    "Within the hard and soft margin problems, there are two types of problems to consider: primal and dual.\n",
    "\n",
    "## Primal Problems\n",
    "\n",
    "The primal problem, the primal problem is more computationally efficient when the dimension of the data is significantly less than the number of observations. The objective function for the primal problem looks like this:\n",
    "\n",
    "$\\text{argmin}_{w,b,\\xi} \\frac{1}{2}||w||^2 + C\\sum^{N}_{i=1}\\xi_{i}$    \n",
    "subject to $y_{i}(w^T\\phi(x_i)+b) \\ge 1-\\xi \\forall i\\in[1,N]$ and $\\xi_{i}\\ge 0 \\forall i\\in [1,N]$.\n",
    "\n",
    "Here, $C > 0$ is a hyperparameter defined in the soft margin problem. As $C$ nears infinity, the model behaves more like a hard margin problem. With regards to $\\xi$, if all $\\xi_{i} = 0$, it is a hard margin problem. Only if there exists some $\\xi_{i} > 0$ is it a soft margin. $N$ is the number of data points in the training set and $\\phi$ is a fixed feature-space transformation that increases the dimensionality of $x$.\n",
    "\n",
    "## Dual Problems\n",
    "\n",
    "Unlike the primal problem, the dual problem is more efficient when the number of observations is significantly less than the dimension of the data. This is the case where SVMs stand out most from other algorithms. There is an important bit of notation to be aware of here: $\\alpha_{i}$ is an index into the vector $\\alpha$ and $y_{(i)}$ is the $i$ index into the dataset. The objective function for the dual problem looks like:\n",
    "\n",
    "$\\text{argmax}_{\\alpha} \\sum^{N}_{i=1}\\alpha_{i} - \\frac{1}{2}\\sum^{N}_{i=1}\\sum^{N}_{j=1}\\alpha_{i}\\alpha_{j}y_{(i)}y_{(j)}\\frac{\\phi(x_{(i)})^{T}\\phi(x_{(j)})}{K(x_{(i)},x_{(j)})}$    \n",
    "subject to $0 \\le \\alpha_{i}\\le C \\forall i\\in[1,N]$ and $\\sum^{N}_{i=1}\\alpha_{i}y_{(i)} = 0$.\n",
    "\n",
    "Here we are maximizing each $\\alpha_{i}$ individually as if its a set of $N$ scalars, not maximizing $||\\alpha||$. It's worth noting that unlike in the primal problem, there is no $\\xi$ but $C$ still shows up in the constraints. $K$ is a kernel function that we will talk about in the next section. Once we have our maximized $\\alpha$, we are able to solve for $w$ and $b$.\n",
    "\n",
    "$w = \\sum^{N}_{i=1}\\alpha_{i}y_{(i)}x_{(i)}$    \n",
    "$b = y_{(i)} - w^{T}x_{(i)}$\n",
    "\n",
    "In the dual problem, $\\alpha_{i}>0$ if and only if $x_{(i)}$ is on the margin (i.e. a support vector). In general, you will find that $\\alpha$ is sparse since there are typically only a few support vectors. Also, while $w$ and $b$ are unique (convex optimization), $\\alpha$ is not guaranteed to be unique.\n",
    "\n",
    "A key names:\n",
    "- **Margin Support Vectors**: $\\alpha_{i}>0$ and $\\xi_{i}=0$\n",
    "- **Non-Margin Support Vectors**: $\\alpha_{i}>0$ and $\\xi_{i}>0$\n",
    "- **Non-Support Vectors**: $\\alpha_{i}=0$ and $\\xi_{i}=0$\n",
    "\n",
    "\n",
    "## Kernel Functions\n",
    "\n",
    "The purpose of a kernel function is to take the training data and project it into higher dimensional space, typically for the purpose of making a non-linear decision boundary. A few of the flavors include:\n",
    "\n",
    "- **Linear Kernel**: Used for a linear boundary.    \n",
    "$K(x,z) = x^(T)z$\n",
    "\n",
    "- **Polynomial Kernel**: Used for creading a $d$-order polynomial expansion.    \n",
    "$K(x,z) = (x^{T}z + 1)^{d}$, where $d$ is a hyperparameter.\n",
    "\n",
    "- **Gaussian Radial Basis Function Kernel (RBF)**: Used for mapping points into an infinite number of dimensions.    \n",
    "$K(x,z) = exp\\{\\frac{-||x-z||^{2}_{2}}{2\\sigma^{2}} \\}$, where $\\sigma$ is a hyperparameter.\n",
    "\n",
    "## Multiclass SVMs for C classes\n",
    "Although I said that SVMs are binary classifiers, there are ways to make it work for multiple classes.\n",
    "- **One vs. Rest**: Train $C$ binary SVMs where each one is class $i$ versus all other classes. The drawbacks of this are that you have to train $C$ models on all $N$ data points, which is computationally expensive, and the classes are highly imbalanced.\n",
    "\n",
    "- **One vs. One**: Train $\\frac{C(C-1)}{2}$ binary SVMs, i.e. every possible pairing of classes. Classify test points based on which class has the majority. The drawbacks of this are that it is a lot of classifiers to train, making it even more inefficient. Also, it requires a decision on how to handle ties \n",
    "\n",
    "## References\n",
    "\n",
    "- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "- [Image](https://datascience.stackexchange.com/questions/43180/maximize-the-margin-formula-in-support-vector-machines-algorithm)\n",
    "\n",
    "## Example using `SVC` from `skikit-learn`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
